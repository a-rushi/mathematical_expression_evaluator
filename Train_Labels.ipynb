{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision.transforms import ToTensor\n",
    "from PIL import Image\n",
    "import csv\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import *\n",
    "import time\n",
    "import copy\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from skimage.transform import resize\n",
    "import imageio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = \"/Users/rushi/Downloads/SoML-50/data/\"\n",
    "annotations = \"/Users/rushi/Downloads/SoML-50/annotations.csv\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShauryaDataset(Dataset):\r\n",
    "    def __init__(self,root,annotations,transform=ToTensor()):\r\n",
    "        self.root = root\r\n",
    "        self.transform = transform\r\n",
    "        rows = []\r\n",
    "        with open(annotations,mode='r') as csvfile:\r\n",
    "            csvreader = csv.reader(csvfile)\r\n",
    "            fields = next(csvreader)\r\n",
    "            for row in csvreader:\r\n",
    "                rows.append(row)\r\n",
    "        self.data = rows\r\n",
    "        \r\n",
    "\r\n",
    "    def __len__(self):\r\n",
    "        return len(self.data)\r\n",
    "\r\n",
    "    def __getitem__(self, idx):\r\n",
    "        image_path = self.root + self.data[idx][0]\r\n",
    "        image = Image.open(image_path)\r\n",
    "        image = ToTensor()(image).squeeze(0)\r\n",
    "        image = resize(image,(28,28*3),mode = 'constant')\r\n",
    "        img1 = image[:,:28]\r\n",
    "        img2 = image[:,28:56]\r\n",
    "        img3 = image[:,56:]\r\n",
    "        img1 = img1.reshape(28*28)\r\n",
    "        img2 = img2.reshape(28*28)\r\n",
    "        img3 = img3.reshape(28*28)\r\n",
    "        temp = self.data[idx][1]\r\n",
    "        #y = (int)(self.data[idx][2])+9\r\n",
    "        if(temp == \"prefix\"):\r\n",
    "            y = 0\r\n",
    "            return image,img1,img2,img3,y\r\n",
    "        elif(temp == \"infix\"):\r\n",
    "            y = 1\r\n",
    "            return image,img2,img1,img3,y\r\n",
    "        else:\r\n",
    "            y = 2\r\n",
    "            return image,img3,img2,img1,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ShauryaDataset(root,annotations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import random_split\r\n",
    "dataset_train, dataset_val, dataset_test = random_split(dataset,[40000,5000,5000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "batchsize = 100\n",
    "dataset_new = DataLoader(dataset,len(dataset),shuffle = False)\n",
    "print(len(dataset_new))\n",
    "train_loader = DataLoader(dataset_test,batchsize,shuffle=False)\n",
    "val_loader = DataLoader(dataset_val,batchsize,shuffle=False)\n",
    "inputsize = 28*84\n",
    "classes = 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(outputs, labels):\n",
    "    _, preds = torch.max(outputs, dim=1)\n",
    "    return torch.tensor(torch.sum(preds == labels).item() / len(preds))\n",
    "\n",
    "def evaluate(model, val_loader):\n",
    "    outputs = [model.validation_step(batch) for batch in val_loader]\n",
    "    return model.validation_epoch_end(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net_LABELS(nn.Module):   \n",
    "    def __init__(self):\n",
    "        super(Net_LABELS, self).__init__()\n",
    "        self.cnn_layers = nn.Sequential(\n",
    "            nn.Conv2d(1, 4, kernel_size=5, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(4),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "            nn.Conv2d(4, 4, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(4),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        )\n",
    "\n",
    "        self.linear = nn.Sequential(\n",
    "            nn.Linear(4*6*20,classes),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "\n",
    "    # Defining the forward pass    \n",
    "    def forward(self, x):\n",
    "        x = x.reshape(-1,1,28,84)\n",
    "        x = self.cnn_layers(x)\n",
    "        x = x.reshape(-1,4*6*20)\n",
    "        x = self.linear(x)\n",
    "        return x\n",
    "\n",
    "    def training_step(self, batch):\n",
    "        images,img1,img2,img3, labels = batch\n",
    "        out = self(images)                  # Generate predictions\n",
    "        loss = F.cross_entropy(out, labels) # Calculate loss\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch):\n",
    "        images,img1,img2,img3, labels = batch \n",
    "        out = self(images)                    # Generate predictions\n",
    "        loss = F.cross_entropy(out, labels)   # Calculate loss\n",
    "        acc = accuracy(out, labels)           # Calculate accuracy\n",
    "        return {'val_loss': loss, 'val_acc': acc}\n",
    "        \n",
    "    def validation_epoch_end(self, outputs):\n",
    "        batch_losses = [x['val_loss'] for x in outputs]\n",
    "        epoch_loss = torch.stack(batch_losses).mean()   # Combine losses\n",
    "        batch_accs = [x['val_acc'] for x in outputs]\n",
    "        epoch_acc = torch.stack(batch_accs).mean()      # Combine accuracies\n",
    "        return {'val_loss': epoch_loss.item(), 'val_acc': epoch_acc.item()}\n",
    "    \n",
    "    def epoch_end(self, epoch, result):\n",
    "        print(\"Epoch [{}], val_loss: {:.4f}, val_acc: {:.4f}\".format(epoch, result['val_loss'], result['val_acc']))\n",
    "    \n",
    "\n",
    "model = Net_LABELS()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(epochs, lr, model, train_loader, val_loader, opt_func=torch.optim.SGD):\r\n",
    "    optimizer = opt_func(model.parameters(), lr)\r\n",
    "    history = []\r\n",
    "\r\n",
    "    for epoch in range(epochs):\r\n",
    "\r\n",
    "        for batch in train_loader:\r\n",
    "            loss = model.training_step(batch)\r\n",
    "            loss.backward()\r\n",
    "            optimizer.step()\r\n",
    "            optimizer.zero_grad()\r\n",
    "\r\n",
    "        result = evaluate(model, val_loader)\r\n",
    "        model.epoch_end(epoch, result)\r\n",
    "        history.append(result)\r\n",
    "\r\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rushi/opt/anaconda3/lib/python3.7/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  ../c10/core/TensorImpl.h:1156.)\n",
      "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0], val_loss: 0.5279, val_acc: 0.8170\n",
      "Epoch [1], val_loss: 0.3765, val_acc: 0.8678\n",
      "Epoch [2], val_loss: 0.2980, val_acc: 0.8980\n",
      "Epoch [3], val_loss: 0.2509, val_acc: 0.9152\n",
      "Epoch [4], val_loss: 0.2174, val_acc: 0.9240\n",
      "Epoch [5], val_loss: 0.1918, val_acc: 0.9346\n",
      "Epoch [6], val_loss: 0.1717, val_acc: 0.9428\n",
      "Epoch [7], val_loss: 0.1559, val_acc: 0.9462\n",
      "Epoch [8], val_loss: 0.1443, val_acc: 0.9494\n",
      "Epoch [9], val_loss: 0.1352, val_acc: 0.9518\n",
      "Epoch [10], val_loss: 0.1280, val_acc: 0.9538\n",
      "Epoch [11], val_loss: 0.1221, val_acc: 0.9546\n",
      "Epoch [12], val_loss: 0.1170, val_acc: 0.9572\n",
      "Epoch [13], val_loss: 0.1126, val_acc: 0.9586\n",
      "Epoch [14], val_loss: 0.1088, val_acc: 0.9614\n",
      "Epoch [15], val_loss: 0.1054, val_acc: 0.9626\n",
      "Epoch [16], val_loss: 0.1022, val_acc: 0.9630\n",
      "Epoch [17], val_loss: 0.0994, val_acc: 0.9638\n",
      "Epoch [18], val_loss: 0.0967, val_acc: 0.9658\n",
      "Epoch [19], val_loss: 0.0943, val_acc: 0.9668\n",
      "Epoch [20], val_loss: 0.0920, val_acc: 0.9680\n",
      "Epoch [21], val_loss: 0.0899, val_acc: 0.9684\n",
      "Epoch [22], val_loss: 0.0879, val_acc: 0.9688\n",
      "Epoch [23], val_loss: 0.0861, val_acc: 0.9696\n",
      "Epoch [24], val_loss: 0.0845, val_acc: 0.9700\n",
      "Epoch [25], val_loss: 0.0830, val_acc: 0.9704\n",
      "Epoch [26], val_loss: 0.0816, val_acc: 0.9712\n",
      "Epoch [27], val_loss: 0.0803, val_acc: 0.9708\n",
      "Epoch [28], val_loss: 0.0788, val_acc: 0.9714\n",
      "Epoch [29], val_loss: 0.0777, val_acc: 0.9714\n",
      "Epoch [30], val_loss: 0.0765, val_acc: 0.9714\n",
      "Epoch [31], val_loss: 0.0754, val_acc: 0.9720\n",
      "Epoch [32], val_loss: 0.0743, val_acc: 0.9722\n",
      "Epoch [33], val_loss: 0.0734, val_acc: 0.9730\n",
      "Epoch [34], val_loss: 0.0725, val_acc: 0.9740\n",
      "Epoch [35], val_loss: 0.0716, val_acc: 0.9740\n",
      "Epoch [36], val_loss: 0.0708, val_acc: 0.9742\n",
      "Epoch [37], val_loss: 0.0701, val_acc: 0.9744\n",
      "Epoch [38], val_loss: 0.0693, val_acc: 0.9744\n",
      "Epoch [39], val_loss: 0.0686, val_acc: 0.9750\n",
      "Epoch [40], val_loss: 0.0680, val_acc: 0.9750\n",
      "Epoch [41], val_loss: 0.0673, val_acc: 0.9750\n",
      "Epoch [42], val_loss: 0.0667, val_acc: 0.9746\n",
      "Epoch [43], val_loss: 0.0661, val_acc: 0.9750\n"
     ]
    }
   ],
   "source": [
    "fit(100,0.01,model,train_loader,val_loader)\n",
    "torch.save(model.state_dict(),'Last_hope_labels.pt')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "689468ff578cf1937b9f318807e698bee6ce88f59da82529917ffa4402cb232f"
  },
  "kernelspec": {
   "display_name": "Python 3.7.10 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}