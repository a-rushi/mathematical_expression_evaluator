{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "from torchvision.datasets import MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\r\n",
    "import torch.nn as nn\r\n",
    "import torch.nn.functional as F\r\n",
    "from torch.utils.data import DataLoader\r\n",
    "from torch.utils.data import Dataset\r\n",
    "from torchvision.transforms import ToTensor\r\n",
    "from PIL import Image\r\n",
    "import csv\r\n",
    "import numpy as np\r\n",
    "import matplotlib.pyplot as plt\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "%matplotlib inline\r\n",
    "import torchvision.transforms as transforms\r\n",
    "from skimage.transform import resize\r\n",
    "import imageio\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import csv\n",
    "mypath = \"/Users/rushi/Downloads/SoML-50/train/minus/\"\n",
    "subt_train = [f for f in listdir(mypath) if isfile(join(mypath, f))]\n",
    "mypath = \"/Users/rushi/Downloads/SoML-50/train/plus/\"\n",
    "addn_train = [f for f in listdir(mypath) if isfile(join(mypath, f))]\n",
    "mypath = \"/Users/rushi/Downloads/SoML-50/train/times/\"\n",
    "mult_train = [f for f in listdir(mypath) if isfile(join(mypath, f))]\n",
    "mypath = \"/Users/rushi/Downloads/SoML-50/train/div/\"\n",
    "divn_train = [f for f in listdir(mypath) if isfile(join(mypath, f))]\n",
    "\n",
    "\n",
    "mypath = \"/Users/rushi/Downloads/SoML-50/eval/minus/\"\n",
    "subt_test = [f for f in listdir(mypath) if isfile(join(mypath, f))]\n",
    "mypath = \"/Users/rushi/Downloads/SoML-50/eval/plus/\"\n",
    "addn_test = [f for f in listdir(mypath) if isfile(join(mypath, f))]\n",
    "mypath = \"/Users/rushi/Downloads/SoML-50/eval/times/\"\n",
    "mult_test = [f for f in listdir(mypath) if isfile(join(mypath, f))]\n",
    "mypath = \"/Users/rushi/Downloads/SoML-50/eval/div/\"\n",
    "divn_test = [f for f in listdir(mypath) if isfile(join(mypath, f))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Subtraction -> 0 ; Addition -> 1; Multiplication -> 2; Division -> 3\n",
    "\n",
    "rows = []\n",
    "\n",
    "fields = ['File','Label']\n",
    "\n",
    "for f in subt_train:\n",
    "    rows.append([\"/Users/rushi/Downloads/SoML-50/train/minus/\" + f,'0'])\n",
    "for f in addn_train:\n",
    "    rows.append([\"/Users/rushi/Downloads/SoML-50/train/plus/\" + f,'1'])\n",
    "for f in mult_train:\n",
    "    rows.append([\"/Users/rushi/Downloads/SoML-50/train/times/\" + f,'2'])\n",
    "for f in divn_train:\n",
    "    rows.append([\"/Users/rushi/Downloads/SoML-50/train/div/\" + f,'3'])\n",
    "for f in subt_test:\n",
    "    rows.append([\"/Users/rushi/Downloads/SoML-50/eval/minus/\" + f,'0'])\n",
    "for f in addn_test:\n",
    "    rows.append([\"/Users/rushi/Downloads/SoML-50/eval/plus/\"+ f,'1'])\n",
    "for f in mult_test:\n",
    "    rows.append([\"/Users/rushi/Downloads/SoML-50/eval/times/\" + f,'2'])\n",
    "for f in divn_test:\n",
    "    rows.append([\"/Users/rushi/Downloads/SoML-50/eval/div/\" + f,'3'])\n",
    "    \n",
    "filename = \"datavals.csv\"\n",
    "with open(filename,'w',newline = '') as csvfile:\n",
    "    csvwriter = csv.writer(csvfile)\n",
    "    csvwriter.writerow(fields) \n",
    "    csvwriter.writerows(rows)\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = \"/Users/rushi/Downloads/SoML-50/\"\n",
    "annotations = \"/Users/rushi/Downloads/datavals.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShravanDataset(Dataset):\n",
    "    def __init__(self,root,annotations,transform=ToTensor()):\n",
    "        self.root = root\n",
    "        self.transform = transform\n",
    "        rows = []\n",
    "        with open(annotations,mode='r') as csvfile:\n",
    "            csvreader = csv.reader(csvfile)\n",
    "            fields = next(csvreader)\n",
    "            for row in csvreader:\n",
    "                rows.append(row)\n",
    "        self.data = rows\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path = self.data[idx][0]\n",
    "        image = Image.open(image_path)\n",
    "        image = ToTensor()(image).squeeze(0)\n",
    "        image_new = image[0]*0.2989 + image[1]*0.5870 + image[2]*0.1140\n",
    "        image_new = resize(image_new,(28,28),mode = 'constant')\n",
    "        y = (int)(self.data[idx][1])\n",
    "        return image_new,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ShravanDataset(root,annotations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from torch.utils.data import random_split\n",
    "#train_dataset, val_dataset = random_split(dataset, [2000,509])\n",
    "#print(dataset[0])\n",
    "batchsize = 40\n",
    "#from torch.utils.data import random_split\n",
    "train_loader = DataLoader(dataset,batchsize,shuffle=False)\n",
    "#train_dataset, val_dataset = random_split(dataset, [60000,0])\n",
    "batchsize = 100\n",
    "#val_loader = DataLoader(val_dataset,batchsize,shuffle=False)\n",
    "input_size = 28*28\n",
    "num_classes = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net_OPER(nn.Module):   \n",
    "    def __init__(self):\n",
    "        super(Net_OPER, self).__init__()\n",
    "        self.cnn_layers = nn.Sequential(\n",
    "            nn.Conv2d(1, 4, kernel_size=5, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(4),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "            nn.Conv2d(4, 4, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(4),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        )\n",
    "\n",
    "        self.linear = nn.Sequential(\n",
    "            nn.Linear(4*6*6,num_classes),\n",
    "            nn.Softmax(),\n",
    "        )\n",
    "\n",
    "\n",
    "    # Defining the forward pass    \n",
    "    def forward(self, x):\n",
    "        #print(x.shape)\n",
    "        x = x.reshape(-1,1,28,28)\n",
    "        #print(x.shape)\n",
    "        x = self.cnn_layers(x)\n",
    "        #print(x.shape)\n",
    "        x = x.reshape(-1,4*6*6)\n",
    "        x = self.linear(x)\n",
    "        return x\n",
    "\n",
    "    def training_step(self, batch):\n",
    "        images,labels = batch\n",
    "        out = self(images)                  # Generate predictions\n",
    "        loss = F.cross_entropy(out, labels) # Calculate loss\n",
    "        return loss\n",
    "    \"\"\"\n",
    "    def validation_step(self, batch):\n",
    "        images,labels = batch \n",
    "        out = self(images)                    # Generate predictions\n",
    "        loss = F.cross_entropy(out, labels)   # Calculate loss\n",
    "        acc = accuracy(out, labels)           # Calculate accuracy\n",
    "        return {'val_loss': loss, 'val_acc': acc}\n",
    "        \n",
    "    def validation_epoch_end(self, outputs):\n",
    "        batch_losses = [x['val_loss'] for x in outputs]\n",
    "        epoch_loss = torch.stack(batch_losses).mean()   # Combine losses\n",
    "        batch_accs = [x['val_acc'] for x in outputs]\n",
    "        epoch_acc = torch.stack(batch_accs).mean()      # Combine accuracies\n",
    "        return {'val_loss': epoch_loss.item(), 'val_acc': epoch_acc.item()}\n",
    "    \n",
    "    def epoch_end(self, epoch, result):\n",
    "        print(\"Epoch [{}], val_loss: {:.4f}, val_acc: {:.4f}\".format(epoch, result['val_loss'], result['val_acc']))\n",
    "    \"\"\"\n",
    "\n",
    "model = Net_OPER()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(outputs, labels):\n",
    "    _, preds = torch.max(outputs, dim=1)\n",
    "    return torch.tensor(torch.sum(preds == labels).item() / len(preds))\n",
    "\n",
    "#loss_fn = F.cross_entropy\n",
    "\"\"\"\n",
    "def evaluate(model, val_loader):\n",
    "    outputs = [model.validation_step(batch) for batch in val_loader]\n",
    "    return model.validation_epoch_end(outputs)\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n        result = evaluate(model, val_loader)\\n        model.epoch_end(epoch, result)\\n        history.append(result)\\n'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def fit(epochs, lr, model, train_loader,opt_func=torch.optim.SGD):\n",
    "    optimizer = opt_func(model.parameters(), lr)\n",
    "    history = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        for batch in train_loader:\n",
    "            loss = model.training_step(batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "    return history\n",
    "\"\"\"\n",
    "        result = evaluate(model, val_loader)\n",
    "        model.epoch_end(epoch, result)\n",
    "        history.append(result)\n",
    "\"\"\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rushi/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/container.py:139: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  input = module(input)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fit(100,0.01,model,train_loader)\n",
    "torch.save(model.state_dict(),'Test_oper.pt')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "689468ff578cf1937b9f318807e698bee6ce88f59da82529917ffa4402cb232f"
  },
  "kernelspec": {
   "display_name": "Python 3.7.10 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}