{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from PIL import Image\n",
    "from torchvision.transforms import ToTensor\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "from torchvision.datasets import MNIST\n",
    "from skimage.transform import resize\n",
    "import imageio\n",
    "import numpy as np\n",
    "import cv2\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "batchsize = 100\n",
    "inputsize = 28*84\n",
    "intermediate_nodes1 = 400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShauryaDataset(Dataset):\n",
    "    def __init__(self,root,subt_train,transform=ToTensor()):\n",
    "        if(subt_train != '.DS_Store'):\n",
    "            self.root = root\n",
    "            self.transform = transform\n",
    "            self.data = subt_train\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path = root + self.data[idx]\n",
    "        image = Image.open(image_path)\n",
    "        image = ToTensor()(image).squeeze(0)\n",
    "        image = resize(image,(28,28*3),mode = 'constant')\n",
    "        img1 = image[:,:28]\n",
    "        img2 = image[:,28:56]\n",
    "        img3 = image[:,56:]\n",
    "        img1 = img1.reshape(28*28)\n",
    "        img2 = img2.reshape(28*28)\n",
    "        img3 = img3.reshape(28*28)\n",
    "        return image,img1,img2,img3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mypath = '/Users/rushi/Downloads/SoML-50/data/'\n",
    "mypath = input(\"Enter testing data path:\")\n",
    "subt_train = [f for f in listdir(mypath) if isfile(join(mypath, f))]\n",
    "root = mypath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ShauryaDataset(root,subt_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes_labels = 3\n",
    "class Net_LABELS(nn.Module):   \n",
    "    def __init__(self):\n",
    "        super(Net_LABELS, self).__init__()\n",
    "        self.cnn_layers = nn.Sequential(\n",
    "            nn.Conv2d(1, 4, kernel_size=5, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(4),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "            nn.Conv2d(4, 4, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(4),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        )\n",
    "\n",
    "        self.linear = nn.Sequential(\n",
    "            nn.Linear(4*6*20,num_classes_labels),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "\n",
    "    # Defining the forward pass    \n",
    "    def forward(self, x):\n",
    "        x = x.reshape(-1,1,28,84)\n",
    "        x = self.cnn_layers(x)\n",
    "        x = x.reshape(-1,4*6*20)\n",
    "        x = self.linear(x)\n",
    "        return x\n",
    "\n",
    "    def training_step(self, batch):\n",
    "        images,img1,img2,img3, labels = batch\n",
    "        out = self(images)                  # Generate predictions\n",
    "        loss = F.cross_entropy(out, labels) # Calculate loss\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch):\n",
    "        images,img1,img2,img3, labels = batch \n",
    "        out = self(images)                    # Generate predictions\n",
    "        loss = F.cross_entropy(out, labels)   # Calculate loss\n",
    "        acc = accuracy(out, labels)           # Calculate accuracy\n",
    "        return {'val_loss': loss, 'val_acc': acc}\n",
    "        \n",
    "    def validation_epoch_end(self, outputs):\n",
    "        batch_losses = [x['val_loss'] for x in outputs]\n",
    "        epoch_loss = torch.stack(batch_losses).mean()   # Combine losses\n",
    "        batch_accs = [x['val_acc'] for x in outputs]\n",
    "        epoch_acc = torch.stack(batch_accs).mean()      # Combine accuracies\n",
    "        return {'val_loss': epoch_loss.item(), 'val_acc': epoch_acc.item()}\n",
    "    \n",
    "    def epoch_end(self, epoch, result):\n",
    "        print(\"Epoch [{}], val_loss: {:.4f}, val_acc: {:.4f}\".format(epoch, result['val_loss'], result['val_acc']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_dataset = DataLoader(dataset,len(dataset))\n",
    "for batch in new_dataset:\n",
    "    img,img1,img2,img3 = batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_Labels = Net_LABELS()\n",
    "model_Labels.load_state_dict(torch.load('Last_hope_labels.pt'))\n",
    "model_Labels.eval()\n",
    "results_labels = torch.argmax(model_Labels(img),dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"Apar_wink_wink_1.csv\"\n",
    "fields = []\n",
    "fields.append(\"Image_Name\")\n",
    "fields.append(\"Label\")\n",
    "rows = []\n",
    "with open(filename,'w',newline = '') as csvfile:\n",
    "    csvwriter = csv.writer(csvfile)\n",
    "    csvwriter.writerow(fields) \n",
    "    for i in range(len(dataset)):\n",
    "        row = []\n",
    "        row.append(subt_train[i])\n",
    "        if(results_labels[i] == 0):\n",
    "            row.append(\"prefix\")\n",
    "        elif(results_labels[i] == 1):\n",
    "            row.append(\"infix\")\n",
    "        else:\n",
    "            row.append(\"postfix\")\n",
    "        rows.append(row)\n",
    "    csvwriter.writerows(rows)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "689468ff578cf1937b9f318807e698bee6ce88f59da82529917ffa4402cb232f"
  },
  "kernelspec": {
   "display_name": "Python 3.7.10 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}