{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from PIL import Image\n",
    "from torchvision.transforms import ToTensor\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "from torchvision.datasets import MNIST\n",
    "from skimage.transform import resize\n",
    "import imageio\n",
    "import numpy as np\n",
    "import cv2\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "batchsize = 100\n",
    "inputsize = 28*84\n",
    "intermediate_nodes1 = 400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShauryaDataset(Dataset):\n",
    "    def __init__(self,root,subt_train,transform=ToTensor()):\n",
    "        if(subt_train != '.DS_Store'):\n",
    "            self.root = root\n",
    "            self.transform = transform\n",
    "            self.data = subt_train\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path = root + self.data[idx]\n",
    "        image = Image.open(image_path)\n",
    "        image = ToTensor()(image).squeeze(0)\n",
    "        image = resize(image,(28,28*3),mode = 'constant')\n",
    "        img1 = image[:,:28]\n",
    "        img2 = image[:,28:56]\n",
    "        img3 = image[:,56:]\n",
    "        img1 = img1.reshape(28*28)\n",
    "        img2 = img2.reshape(28*28)\n",
    "        img3 = img3.reshape(28*28)\n",
    "        return image,img1,img2,img3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mypath = '/Users/rushi/Downloads/SoML-50/data/'\n",
    "mypath = input(\"Enter testing data path:\")\n",
    "subt_train = [f for f in listdir(mypath) if isfile(join(mypath, f))]\n",
    "root = mypath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ShauryaDataset(root,subt_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes_labels = 3\n",
    "class Net_LABELS(nn.Module):   \n",
    "    def __init__(self):\n",
    "        super(Net_LABELS, self).__init__()\n",
    "        self.cnn_layers = nn.Sequential(\n",
    "            nn.Conv2d(1, 4, kernel_size=5, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(4),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "            nn.Conv2d(4, 4, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(4),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        )\n",
    "\n",
    "        self.linear = nn.Sequential(\n",
    "            nn.Linear(4*6*20,num_classes_labels),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "\n",
    "    # Defining the forward pass    \n",
    "    def forward(self, x):\n",
    "        x = x.reshape(-1,1,28,84)\n",
    "        x = self.cnn_layers(x)\n",
    "        x = x.reshape(-1,4*6*20)\n",
    "        x = self.linear(x)\n",
    "        return x\n",
    "\n",
    "    def training_step(self, batch):\n",
    "        images,img1,img2,img3, labels = batch\n",
    "        out = self(images)                  # Generate predictions\n",
    "        loss = F.cross_entropy(out, labels) # Calculate loss\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch):\n",
    "        images,img1,img2,img3, labels = batch \n",
    "        out = self(images)                    # Generate predictions\n",
    "        loss = F.cross_entropy(out, labels)   # Calculate loss\n",
    "        acc = accuracy(out, labels)           # Calculate accuracy\n",
    "        return {'val_loss': loss, 'val_acc': acc}\n",
    "        \n",
    "    def validation_epoch_end(self, outputs):\n",
    "        batch_losses = [x['val_loss'] for x in outputs]\n",
    "        epoch_loss = torch.stack(batch_losses).mean()   # Combine losses\n",
    "        batch_accs = [x['val_acc'] for x in outputs]\n",
    "        epoch_acc = torch.stack(batch_accs).mean()      # Combine accuracies\n",
    "        return {'val_loss': epoch_loss.item(), 'val_acc': epoch_acc.item()}\n",
    "    \n",
    "    def epoch_end(self, epoch, result):\n",
    "        print(\"Epoch [{}], val_loss: {:.4f}, val_acc: {:.4f}\".format(epoch, result['val_loss'], result['val_acc']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes_digits = 10\n",
    "class Net_DIGITS(nn.Module):   \n",
    "    def __init__(self):\n",
    "        super(Net_DIGITS, self).__init__()\n",
    "        self.cnn_layers = nn.Sequential(\n",
    "            nn.Conv2d(1, 4, kernel_size=5, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(4),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "            nn.Conv2d(4, 4, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(4),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        )\n",
    "\n",
    "        self.linear = nn.Sequential(\n",
    "            nn.Linear(4*6*6,num_classes_digits),\n",
    "            nn.Softmax(),\n",
    "        )\n",
    "\n",
    "\n",
    "    # Defining the forward pass    \n",
    "    def forward(self, x):\n",
    "        #print(x.shape)\n",
    "        x = x.reshape(-1,1,28,28)\n",
    "        #print(torch.is_tensor(x))\n",
    "        x = self.cnn_layers(x)\n",
    "        #print(x.shape)\n",
    "        x = x.reshape(-1,4*6*6)\n",
    "        x = self.linear(x)\n",
    "        return x\n",
    "\n",
    "    def training_step(self, batch):\n",
    "        images,labels = batch\n",
    "        out = self(images)                  # Generate predictions\n",
    "        loss = F.cross_entropy(out, labels) # Calculate loss\n",
    "        return loss\n",
    "    \"\"\"\"\n",
    "    def validation_step(self, batch):\n",
    "        images,labels = batch \n",
    "        out = self(images)                    # Generate predictions\n",
    "        loss = F.cross_entropy(out, labels)   # Calculate loss\n",
    "        acc = accuracy(out, labels)           # Calculate accuracy\n",
    "        return {'val_loss': loss, 'val_acc': acc}\n",
    "        \n",
    "    def validation_epoch_end(self, outputs):\n",
    "        batch_losses = [x['val_loss'] for x in outputs]\n",
    "        epoch_loss = torch.stack(batch_losses).mean()   # Combine losses\n",
    "        batch_accs = [x['val_acc'] for x in outputs]\n",
    "        epoch_acc = torch.stack(batch_accs).mean()      # Combine accuracies\n",
    "        return {'val_loss': epoch_loss.item(), 'val_acc': epoch_acc.item()}\n",
    "    \n",
    "    def epoch_end(self, epoch, result):\n",
    "        print(\"Epoch [{}], val_loss: {:.4f}, val_acc: {:.4f}\".format(epoch, result['val_loss'], result['val_acc']))\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes_oper = 4\n",
    "class Net_OPER(nn.Module):   \n",
    "    def __init__(self):\n",
    "        super(Net_OPER, self).__init__()\n",
    "        self.cnn_layers = nn.Sequential(\n",
    "            nn.Conv2d(1, 4, kernel_size=5, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(4),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "            nn.Conv2d(4, 4, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(4),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        )\n",
    "\n",
    "        self.linear = nn.Sequential(\n",
    "            nn.Linear(4*6*6,num_classes_oper),\n",
    "            nn.Softmax(),\n",
    "        )\n",
    "\n",
    "\n",
    "    # Defining the forward pass    \n",
    "    def forward(self, x):\n",
    "        #print(x.shape)\n",
    "        x = x.reshape(-1,1,28,28)\n",
    "        #print(x.shape)\n",
    "        x = self.cnn_layers(x)\n",
    "        #print(x.shape)\n",
    "        x = x.reshape(-1,4*6*6)\n",
    "        x = self.linear(x)\n",
    "        return x\n",
    "\n",
    "    def training_step(self, batch):\n",
    "        images,labels = batch\n",
    "        out = self(images)                  # Generate predictions\n",
    "        loss = F.cross_entropy(out, labels) # Calculate loss\n",
    "        return loss\n",
    "    \"\"\"\n",
    "    def validation_step(self, batch):\n",
    "        images,labels = batch \n",
    "        out = self(images)                    # Generate predictions\n",
    "        loss = F.cross_entropy(out, labels)   # Calculate loss\n",
    "        acc = accuracy(out, labels)           # Calculate accuracy\n",
    "        return {'val_loss': loss, 'val_acc': acc}\n",
    "        \n",
    "    def validation_epoch_end(self, outputs):\n",
    "        batch_losses = [x['val_loss'] for x in outputs]\n",
    "        epoch_loss = torch.stack(batch_losses).mean()   # Combine losses\n",
    "        batch_accs = [x['val_acc'] for x in outputs]\n",
    "        epoch_acc = torch.stack(batch_accs).mean()      # Combine accuracies\n",
    "        return {'val_loss': epoch_loss.item(), 'val_acc': epoch_acc.item()}\n",
    "    \n",
    "    def epoch_end(self, epoch, result):\n",
    "        print(\"Epoch [{}], val_loss: {:.4f}, val_acc: {:.4f}\".format(epoch, result['val_loss'], result['val_acc']))\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_dataset = DataLoader(dataset,len(dataset))\n",
    "for batch in new_dataset:\n",
    "    img,img1,img2,img3 = batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rushi/opt/anaconda3/lib/python3.7/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  ../c10/core/TensorImpl.h:1156.)\n",
      "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
     ]
    }
   ],
   "source": [
    "model_Labels = Net_LABELS()\n",
    "model_Labels.load_state_dict(torch.load('Last_hope_labels.pt'))\n",
    "model_Labels.eval()\n",
    "results_labels = torch.argmax(model_Labels(img),dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "digits1 = np.zeros((len(dataset),784))\n",
    "digits2 = np.zeros((len(dataset),784))\n",
    "oper = np.zeros((len(dataset),784))\n",
    "idx = 0\n",
    "for idx in range(len(dataset)):\n",
    "    if(results_labels[idx] == 0):\n",
    "        oper[idx] = img1[idx]\n",
    "        digits1[idx] = img2[idx]\n",
    "        digits2[idx] = img3[idx]\n",
    "    elif(results_labels[idx] == 1):\n",
    "        oper[idx] = img2[idx]\n",
    "        digits1[idx] = img1[idx]\n",
    "        digits2[idx] = img3[idx]\n",
    "    else:\n",
    "        oper[idx] = img3[idx]\n",
    "        digits1[idx] = img1[idx]\n",
    "        digits2[idx] = img2[idx]\n",
    "\n",
    "tensor_dig1 = torch.Tensor(digits1)\n",
    "tensor_dig2 = torch.Tensor(digits2)\n",
    "tensor_oper = torch.Tensor(oper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rushi/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/container.py:139: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  input = module(input)\n"
     ]
    }
   ],
   "source": [
    "model_DIGITS = Net_DIGITS()\n",
    "model_DIGITS.load_state_dict(torch.load('Last_hope_dig.pt'))\n",
    "model_DIGITS.eval()\n",
    "results_digits1 = torch.argmax(model_DIGITS(tensor_dig1),dim=1)\n",
    "results_digits2 = torch.argmax(model_DIGITS(tensor_dig2),dim=1)\n",
    "\n",
    "model_oper = Net_OPER()\n",
    "model_oper.load_state_dict(torch.load('Last_hope_oper.pt'))\n",
    "model_oper.eval()\n",
    "results_oper = torch.argmax(model_oper(tensor_oper),dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluations = []\n",
    "for i in range(len(dataset)):\n",
    "    dig1 = results_digits1[i]\n",
    "    dig2 = results_digits2[i]\n",
    "    oper = results_oper[i]\n",
    "    if(oper == 0):\n",
    "        evaluations.append(dig1-dig2)\n",
    "    elif(oper == 1):\n",
    "        evaluations.append(dig1+dig2)\n",
    "    elif(oper == 2):\n",
    "        evaluations.append(dig1*dig2)\n",
    "    else:\n",
    "        if(dig2==0):\n",
    "            dig2 = 8\n",
    "        evaluations.append(dig1/dig2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"Apar_wink_wink_2.csv\"\n",
    "fields = []\n",
    "fields.append(\"Image_Name\")\n",
    "fields.append(\"Value\")\n",
    "rows = []\n",
    "with open(filename,'w',newline = '') as csvfile:\n",
    "    csvwriter = csv.writer(csvfile)\n",
    "    csvwriter.writerow(fields) \n",
    "    for i in range(len(dataset)):\n",
    "        row = []\n",
    "        row.append(subt_train[i])\n",
    "        row.append(int(evaluations[i]))\n",
    "        rows.append(row)\n",
    "    csvwriter.writerows(rows)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "767d51c1340bd893661ea55ea3124f6de3c7a262a8b4abca0554b478b1e2ff90"
  },
  "kernelspec": {
   "display_name": "Python 2.7.16 64-bit",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": ""
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}